#!/usr/bin/env python3
"""
Script 03: LLM Pre-labeling v·ªõi Gemini API

G√°n nh√£n YIELD/HOLD/BACKCHANNEL t·ª± ƒë·ªông cho c√°c segments.

Y√™u c·∫ßu:
    pip install google-generativeai
    export GOOGLE_API_KEY='your_api_key'

Usage:
    python scripts/03_llm_prelabel.py --input data/processed/auto --output data/processed/labeled
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

# Load .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenv not installed

try:
    import google.generativeai as genai
except ImportError:
    print("‚ùå C·∫ßn c√†i ƒë·∫∑t: pip install google-generativeai")
    sys.exit(1)


# Prompt template cho Gemini
SYSTEM_PROMPT = """B·∫°n l√† chuy√™n gia ng√¥n ng·ªØ h·ªçc v·ªÅ h·ªôi tho·∫°i ti·∫øng Vi·ªát.

Nhi·ªám v·ª•: Ph√¢n t√≠ch ƒëo·∫°n h·ªôi tho·∫°i v√† g√°n nh√£n TURN-TAKING cho M·ªñI ph√°t ng√¥n.

3 LO·∫†I NH√ÉN:

1. YIELD (Nh∆∞·ªùng l·ªùi):
   - Ng∆∞·ªùi n√≥i K·∫æT TH√öC l∆∞·ª£t, s·∫µn s√†ng ƒë·ªÉ ng∆∞·ªùi kh√°c n√≥i
   - D·∫•u hi·ªáu: H∆∞ t·ª´ cu·ªëi c√¢u (nh√©, nh·ªâ, √†, h·∫£, ·∫°, hen, nha), gi·ªçng ƒëi xu·ªëng, c√¢u h·ªèi
   - V√≠ d·ª•: "Anh ƒëi ƒë√¢u ƒë·∫•y nh·ªâ?", "Em hi·ªÉu r·ªìi ·∫°"

2. HOLD (Gi·ªØ l·ªùi):
   - Ng∆∞·ªùi n√≥i CH∆ØA XONG, s·∫Ω ti·∫øp t·ª•c
   - D·∫•u hi·ªáu: C√¢u c√≤n dang d·ªü, c√≥ "m√†", "th√¨", "l√†", "v√¨", "nh∆∞ng", gi·ªçng treo
   - V√≠ d·ª•: "T·∫°i v√¨ h√¥m qua...", "Anh nghƒ© l√†..."

3. BACKCHANNEL (Ph·∫£n h·ªìi ng·∫Øn):
   - Ph·∫£n h·ªìi ng·∫Øn KH√îNG chi·∫øm l∆∞·ª£t n√≥i
   - Th∆∞·ªùng ‚â§3 t·ª´, ch·ªâ ƒë·ªÉ th·ªÉ hi·ªán ƒëang nghe
   - V√≠ d·ª•: "·ª´", "v√¢ng", "·ªù", "√†", "th·∫ø √†", "v·∫≠y h·∫£", "ƒë√∫ng r·ªìi"

L∆ØU √ù QUAN TR·ªåNG:
- N·∫øu ph√°t ng√¥n ng·∫Øn (<3 t·ª´) v√† ch·ªâ l√† ph·∫£n h·ªìi ‚Üí BACKCHANNEL
- N·∫øu c√¢u h·ªèi ‚Üí th∆∞·ªùng l√† YIELD
- N·∫øu c√¢u ch∆∞a ho√†n ch·ªânh ‚Üí HOLD"""

USER_PROMPT_TEMPLATE = """Ph√¢n t√≠ch h·ªôi tho·∫°i sau v√† g√°n nh√£n cho T·ª™NG ph√°t ng√¥n:

{conversation}

Tr·∫£ v·ªÅ JSON array v·ªõi format:
[
  {{"segment_id": 0, "label": "YIELD", "confidence": 0.9, "reason": "c√≥ 'nh·ªâ' cu·ªëi c√¢u"}},
  ...
]

CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG C√ì TEXT KH√ÅC."""


class LLMLabeler:
    """LLM-based turn-taking labeler s·ª≠ d·ª•ng Gemini"""
    
    # Rule-based markers cho fallback v√† validation
    YIELD_MARKERS = ['nh√©', 'nh·ªâ', '√†', 'h·∫£', '·∫°', 'hen', 'nha', 'kh√¥ng', 'ch·ª©', 'h·∫£']
    HOLD_MARKERS = ['m√†', 'th√¨', 'l√†', 'v√¨', 'nh∆∞ng', 'n√™n', 'n·∫øu', 'khi', 'r·ªìi']
    BACKCHANNEL_WORDS = ['·ª´', 'v√¢ng', '·ªù', 'd·∫°', '·ª´m', 'ok', 'ƒë∆∞·ª£c']
    
    def __init__(self, model: str = "gemini-2.0-flash", api_key: Optional[str] = None):
        api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("C·∫ßn GOOGLE_API_KEY! Set: export GOOGLE_API_KEY='...'")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
        self.generation_config = {
            "temperature": 0.1,  # Low temperature cho consistency
            "response_mime_type": "application/json"
        }
    
    def _format_conversation(self, segments: List[Dict]) -> str:
        """Format segments th√†nh text cho LLM"""
        lines = []
        for seg in segments:
            speaker = seg.get("speaker", "?")
            text = seg.get("text", "")
            seg_id = seg.get("id", 0)
            lines.append(f"[{seg_id}] {speaker}: \"{text}\"")
        return "\n".join(lines)
    
    def _rule_based_label(self, text: str) -> tuple:
        """Fallback rule-based labeling"""
        text_lower = text.lower().strip()
        words = text_lower.split()
        
        # Backchannel: ng·∫Øn v√† ch·ª©a marker
        if len(words) <= 3:
            if any(bc in text_lower for bc in self.BACKCHANNEL_WORDS):
                return "BACKCHANNEL", 0.8, "rule: short + backchannel word"
        
        # Yield: k·∫øt th√∫c b·∫±ng marker
        if words:
            last_word = words[-1]
            if any(last_word.endswith(ym) for ym in self.YIELD_MARKERS):
                return "YIELD", 0.7, f"rule: ends with '{last_word}'"
        
        # Backchannel: r·∫•t ng·∫Øn
        if len(words) <= 2:
            return "BACKCHANNEL", 0.6, "rule: very short utterance"
        
        # Hold: ch·ª©a hold marker ·ªü gi·ªØa/cu·ªëi
        for hm in self.HOLD_MARKERS:
            if hm in words[-3:]:  # Trong 3 t·ª´ cu·ªëi
                return "HOLD", 0.6, f"rule: contains '{hm}'"
        
        # Default: yield (end of utterance)
        return "YIELD", 0.5, "rule: default"
    
    def label_segments(
        self, 
        segments: List[Dict],
        use_llm: bool = True,
        chunk_size: int = 20
    ) -> List[Dict]:
        """
        G√°n nh√£n cho c√°c segments.
        
        Args:
            segments: List segments t·ª´ whisperX
            use_llm: D√πng LLM hay ch·ªâ rule-based
            chunk_size: S·ªë segments g·ª≠i m·ªói l·∫ßn (tr√°nh token limit)
        """
        if not use_llm:
            # Rule-based only
            for seg in segments:
                label, conf, reason = self._rule_based_label(seg.get("text", ""))
                seg["auto_label"] = label
                seg["confidence"] = conf
                seg["label_reason"] = reason
            return segments
        
        # LLM labeling theo chunks
        for i in range(0, len(segments), chunk_size):
            chunk = segments[i:i + chunk_size]
            
            try:
                # Format conversation
                conv_text = self._format_conversation(chunk)
                prompt = USER_PROMPT_TEMPLATE.format(conversation=conv_text)
                
                # Call API
                response = self.model.generate_content(
                    [
                        {"role": "user", "parts": [SYSTEM_PROMPT]},
                        {"role": "model", "parts": ["T√¥i hi·ªÉu. T√¥i s·∫Ω ph√¢n t√≠ch v√† g√°n nh√£n YIELD/HOLD/BACKCHANNEL cho t·ª´ng ph√°t ng√¥n."]},
                        {"role": "user", "parts": [prompt]}
                    ],
                    generation_config=self.generation_config
                )
                
                # Parse response
                labels = json.loads(response.text)
                
                # Merge labels
                label_map = {l["segment_id"]: l for l in labels}
                for seg in chunk:
                    seg_id = seg["id"]
                    if seg_id in label_map:
                        seg["auto_label"] = label_map[seg_id]["label"]
                        seg["confidence"] = label_map[seg_id].get("confidence", 0.7)
                        seg["label_reason"] = label_map[seg_id].get("reason", "llm")
                    else:
                        # Fallback
                        label, conf, reason = self._rule_based_label(seg.get("text", ""))
                        seg["auto_label"] = label
                        seg["confidence"] = conf
                        seg["label_reason"] = reason
                
                # Rate limit
                time.sleep(0.5)
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  LLM error: {e}. Using rule-based fallback.")
                for seg in chunk:
                    label, conf, reason = self._rule_based_label(seg.get("text", ""))
                    seg["auto_label"] = label
                    seg["confidence"] = conf
                    seg["label_reason"] = reason
        
        return segments
    
    def flag_for_review(self, segments: List[Dict], threshold: float = 0.7) -> List[Dict]:
        """ƒê√°nh d·∫•u segments c·∫ßn human review"""
        for seg in segments:
            needs_review = False
            
            # Low confidence
            if seg.get("confidence", 1) < threshold:
                needs_review = True
            
            # Short text m√† kh√¥ng ph·∫£i backchannel
            text = seg.get("text", "")
            if len(text.split()) <= 2 and seg.get("auto_label") != "BACKCHANNEL":
                needs_review = True
            
            # Long text m√† l√† backchannel
            if len(text.split()) > 5 and seg.get("auto_label") == "BACKCHANNEL":
                needs_review = True
            
            seg["needs_review"] = needs_review
        
        return segments


def process_file(input_path: str, output_path: str, labeler: LLMLabeler, use_llm: bool = True):
    """Process m·ªôt file JSON"""
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    segments = data.get("segments", [])
    
    # Label
    segments = labeler.label_segments(segments, use_llm=use_llm)
    segments = labeler.flag_for_review(segments)
    
    # Stats
    stats = {
        "YIELD": sum(1 for s in segments if s.get("auto_label") == "YIELD"),
        "HOLD": sum(1 for s in segments if s.get("auto_label") == "HOLD"),
        "BACKCHANNEL": sum(1 for s in segments if s.get("auto_label") == "BACKCHANNEL"),
        "needs_review": sum(1 for s in segments if s.get("needs_review"))
    }
    
    data["segments"] = segments
    data["label_stats"] = stats
    
    # Save
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="LLM Pre-labeling cho turn-taking segments"
    )
    
    parser.add_argument("--input", "-i", required=True, help="Input dir/file")
    parser.add_argument("--output", "-o", default="data/processed/labeled", help="Output dir")
    parser.add_argument("--model", default="gemini-2.0-flash", help="Gemini model")
    parser.add_argument("--api-key", help="Google API key (ho·∫∑c set GOOGLE_API_KEY)")
    parser.add_argument("--no-llm", action="store_true", help="Ch·ªâ d√πng rule-based")
    
    args = parser.parse_args()
    
    # Init labeler
    try:
        labeler = LLMLabeler(model=args.model, api_key=args.api_key)
    except ValueError as e:
        print(f"‚ùå {e}")
        sys.exit(1)
    
    input_path = Path(args.input)
    
    if input_path.is_file():
        output_file = Path(args.output) / input_path.name
        print(f"üìù Processing: {input_path.name}")
        stats = process_file(str(input_path), str(output_file), labeler, not args.no_llm)
        print(f"   ‚úÖ {stats}")
    
    elif input_path.is_dir():
        json_files = list(input_path.glob("*.json"))
        print(f"üìÇ Found {len(json_files)} files")
        
        total_stats = {"YIELD": 0, "HOLD": 0, "BACKCHANNEL": 0, "needs_review": 0}
        
        for i, json_file in enumerate(json_files, 1):
            print(f"\n[{i}/{len(json_files)}] {json_file.name}")
            
            output_file = Path(args.output) / json_file.name
            if output_file.exists():
                print("   ‚è≠Ô∏è  Already labeled, skipping")
                continue
            
            stats = process_file(str(json_file), str(output_file), labeler, not args.no_llm)
            print(f"   ‚úÖ {stats}")
            
            for k, v in stats.items():
                total_stats[k] += v
        
        print(f"\nüìä Total: {total_stats}")
    
    else:
        print(f"‚ùå Invalid input: {args.input}")
        sys.exit(1)


if __name__ == "__main__":
    main()
