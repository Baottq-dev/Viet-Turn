#!/usr/bin/env python3
"""
Script 03 v2: LLM Pre-labeling v·ªõi Gemini Multimodal (Audio + Text)

C·∫¢I TI·∫æN so v·ªõi v1:
- G·ª≠i audio chunk + transcript cho Gemini (thay v√¨ ch·ªâ text)
- Ph√¢n t√≠ch ng·ªØ ƒëi·ªáu (rising/falling intonation)
- M·ªü r·ªông taxonomy: COOPERATIVE_INTERRUPT, COMPETITIVE_INTERRUPT

Y√™u c·∫ßu:
    pip install google-generativeai librosa soundfile
    export GOOGLE_API_KEY='your_api_key'

Usage:
    # Multimodal mode (khuy·∫øn ngh·ªã)
    python scripts/03_llm_prelabel_v2.py --input data/processed/auto --audio-dir data/raw --output data/processed/labeled --multimodal
    
    # Text-only mode (fallback)
    python scripts/03_llm_prelabel_v2.py --input data/processed/auto --output data/processed/labeled
"""

import argparse
import json
import os
import sys
import time
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# Load .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    import google.generativeai as genai
except ImportError:
    print("‚ùå C·∫ßn c√†i ƒë·∫∑t: pip install google-generativeai")
    sys.exit(1)

try:
    import librosa
    import soundfile as sf
    import numpy as np
    HAS_AUDIO = True
except ImportError:
    HAS_AUDIO = False
    print("‚ö†Ô∏è librosa/soundfile not installed. Multimodal mode disabled.")


# ============================================================================
# PROMPTS - C·∫£i ti·∫øn cho Multimodal Analysis
# ============================================================================

SYSTEM_PROMPT_MULTIMODAL = """B·∫°n l√† chuy√™n gia ng√¥n ng·ªØ h·ªçc v·ªÅ h·ªôi tho·∫°i ti·∫øng Vi·ªát.

Nhi·ªám v·ª•: NGHE audio ƒë√≠nh k√®m v√† ph√¢n t√≠ch t·ª´ng ph√°t ng√¥n ƒë·ªÉ g√°n nh√£n TURN-TAKING.

5 LO·∫†I NH√ÉN:

1. YIELD (Nh∆∞·ªùng l·ªùi):
   - Ng∆∞·ªùi n√≥i K·∫æT TH√öC l∆∞·ª£t, s·∫µn s√†ng ƒë·ªÉ ng∆∞·ªùi kh√°c n√≥i
   - D·∫•u hi·ªáu √¢m thanh: Gi·ªçng ƒëi XU·ªêNG (falling intonation), c∆∞·ªùng ƒë·ªô gi·∫£m
   - D·∫•u hi·ªáu vƒÉn b·∫£n: H∆∞ t·ª´ cu·ªëi c√¢u (nh√©, nh·ªâ, √†, h·∫£, ·∫°, hen, nha)
   - V√≠ d·ª•: "Anh ƒëi ƒë√¢u ƒë·∫•y nh·ªâ?" (gi·ªçng xu·ªëng)

2. HOLD (Gi·ªØ l·ªùi):
   - Ng∆∞·ªùi n√≥i CH∆ØA XONG, s·∫Ω ti·∫øp t·ª•c
   - D·∫•u hi·ªáu √¢m thanh: Gi·ªçng TREO (kh√¥ng xu·ªëng), c√≥ pause filler (·ªù, √†, ·ª´m)
   - D·∫•u hi·ªáu vƒÉn b·∫£n: C√¢u dang d·ªü, c√≥ "m√†", "th√¨", "l√†", "v√¨", "nh∆∞ng"
   - V√≠ d·ª•: "T·∫°i v√¨ h√¥m qua..." (gi·ªçng treo)

3. BACKCHANNEL (Ph·∫£n h·ªìi ng·∫Øn):
   - Ph·∫£n h·ªìi ng·∫Øn KH√îNG chi·∫øm l∆∞·ª£t n√≥i
   - D·∫•u hi·ªáu √¢m thanh: Gi·ªçng NH·ªé, nhanh, th∆∞·ªùng ch·ªìng l·∫•n
   - D·∫•u hi·ªáu vƒÉn b·∫£n: ‚â§3 t·ª´, ch·ªâ th·ªÉ hi·ªán ƒëang nghe
   - V√≠ d·ª•: "·ª´", "v√¢ng", "th·∫ø √†" (gi·ªçng nh·ªè)

4. COOPERATIVE_INTERRUPT (Ng·∫Øt l·ªùi h·ªó tr·ª£):
   - Ng·∫Øt l·ªùi ƒë·ªÉ H·ªñ TR·ª¢ ng∆∞·ªùi n√≥i
   - D·∫•u hi·ªáu: ƒêi·ªÅn t·ª´ cho ng∆∞·ªùi n√≥i, h·ªèi nhanh ƒë·ªÉ l√†m r√µ
   - V√≠ d·ª•: "C√°i g√¨ c∆°?", "√ù anh l√†...?"

5. COMPETITIVE_INTERRUPT (C∆∞·ªõp l·ªùi):
   - Ng·∫Øt l·ªùi ƒë·ªÉ CHI·∫æM l∆∞·ª£t
   - D·∫•u hi·ªáu √¢m thanh: TƒÉng √¢m l∆∞·ª£ng ƒê·ªòT NG·ªòT
   - D·∫•u hi·ªáu vƒÉn b·∫£n: ƒê·ªïi ch·ªß ƒë·ªÅ, ph·ªß nh·∫≠n
   - V√≠ d·ª•: "Kh√¥ng ph·∫£i ƒë√¢u, th·ª±c ra l√†..." (gi·ªçng to)

PH√ÇN T√çCH √ÇM THANH:
- Nghe k·ªπ NG·ªÆU ƒêI·ªÜU: Gi·ªçng l√™n (rising) hay xu·ªëng (falling)?
- Nghe C∆Ø·ªúNG ƒê·ªò: To hay nh·ªè so v·ªõi context?
- C√≥ CH·ªíNG L·∫§N v·ªõi ng∆∞·ªùi kh√°c kh√¥ng?"""

USER_PROMPT_MULTIMODAL = """Nghe audio ƒë√≠nh k√®m v√† ph√¢n t√≠ch transcript:

{conversation}

V·ªõi M·ªñI ph√°t ng√¥n, h√£y:
1. NGHE ng·ªØ ƒëi·ªáu (l√™n/xu·ªëng/treo)
2. NGHE c∆∞·ªùng ƒë·ªô (to/nh·ªè/b√¨nh th∆∞·ªùng)
3. X√ÅC ƒê·ªäNH c√≥ ch·ªìng l·∫•n kh√¥ng
4. G√ÅN NH√ÉN ph√π h·ª£p

Tr·∫£ v·ªÅ JSON array:
[
  {{"segment_id": 0, "label": "YIELD", "confidence": 0.9, "intonation": "falling", "intensity": "normal", "reason": "gi·ªçng xu·ªëng, c√≥ 'nh·ªâ' cu·ªëi c√¢u"}},
  ...
]

NH√ÉN: YIELD, HOLD, BACKCHANNEL, COOPERATIVE_INTERRUPT, COMPETITIVE_INTERRUPT

CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG C√ì TEXT KH√ÅC."""

# Text-only prompts (fallback)
SYSTEM_PROMPT_TEXT = """B·∫°n l√† chuy√™n gia ng√¥n ng·ªØ h·ªçc v·ªÅ h·ªôi tho·∫°i ti·∫øng Vi·ªát.

Nhi·ªám v·ª•: Ph√¢n t√≠ch vƒÉn b·∫£n h·ªôi tho·∫°i v√† g√°n nh√£n TURN-TAKING cho M·ªñI ph√°t ng√¥n.

5 LO·∫†I NH√ÉN:
1. YIELD - Nh∆∞·ªùng l·ªùi (k·∫øt th√∫c, h∆∞ t·ª´ cu·ªëi: nh√©, nh·ªâ, √†, h·∫£)
2. HOLD - Gi·ªØ l·ªùi (c√¢u dang d·ªü, c√≥: m√†, th√¨, l√†, v√¨)
3. BACKCHANNEL - Ph·∫£n h·ªìi ng·∫Øn (‚â§3 t·ª´: ·ª´, v√¢ng, th·∫ø √†)
4. COOPERATIVE_INTERRUPT - Ng·∫Øt l·ªùi h·ªó tr·ª£ (h·ªèi l√†m r√µ)
5. COMPETITIVE_INTERRUPT - C∆∞·ªõp l·ªùi (ƒë·ªïi ch·ªß ƒë·ªÅ, ph·ªß nh·∫≠n)"""

USER_PROMPT_TEXT = """Ph√¢n t√≠ch h·ªôi tho·∫°i v√† g√°n nh√£n cho T·ª™NG ph√°t ng√¥n:

{conversation}

Tr·∫£ v·ªÅ JSON array:
[
  {{"segment_id": 0, "label": "YIELD", "confidence": 0.9, "reason": "c√≥ 'nh·ªâ' cu·ªëi c√¢u"}},
  ...
]

NH√ÉN: YIELD, HOLD, BACKCHANNEL, COOPERATIVE_INTERRUPT, COMPETITIVE_INTERRUPT
CH·ªà TR·∫¢ V·ªÄ JSON."""


class MultimodalLabeler:
    """LLM-based turn-taking labeler v·ªõi Gemini Multimodal support."""
    
    # Extended markers
    YIELD_MARKERS = ['nh√©', 'nh·ªâ', '√†', 'h·∫£', '·∫°', 'hen', 'nha', 'kh√¥ng', 'ch·ª©']
    HOLD_MARKERS = ['m√†', 'th√¨', 'l√†', 'v√¨', 'nh∆∞ng', 'n√™n', 'n·∫øu', 'khi', 'r·ªìi']
    BACKCHANNEL_WORDS = ['·ª´', 'v√¢ng', '·ªù', 'd·∫°', '·ª´m', 'ok', 'ƒë∆∞·ª£c', 'th·∫ø √†', 'v·∫≠y h·∫£']
    INTERRUPT_MARKERS = ['kh√¥ng ph·∫£i', 'ƒë·ª£i ƒë√£', 'khoan', '√Ω t√¥i l√†']
    
    # Valid labels (extended taxonomy)
    VALID_LABELS = ['YIELD', 'HOLD', 'BACKCHANNEL', 'COOPERATIVE_INTERRUPT', 'COMPETITIVE_INTERRUPT']
    
    def __init__(
        self, 
        model: str = "gemini-1.5-flash",  # 1.5 for audio support
        api_key: Optional[str] = None,
        enable_multimodal: bool = True
    ):
        api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("C·∫ßn GOOGLE_API_KEY! Set: export GOOGLE_API_KEY='...'")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
        self.generation_config = {
            "temperature": 0.1,
            "response_mime_type": "application/json"
        }
        self.enable_multimodal = enable_multimodal and HAS_AUDIO
        self.temp_dir = tempfile.mkdtemp()
        
        if enable_multimodal and not HAS_AUDIO:
            print("‚ö†Ô∏è Multimodal disabled: librosa/soundfile not installed")
    
    def _extract_audio_chunk(
        self,
        audio: np.ndarray,
        sr: int,
        start: float,
        end: float,
        context_padding: float = 1.0  # Add 1s context before/after
    ) -> Tuple[np.ndarray, int]:
        """Extract audio chunk with context padding."""
        # Add padding
        padded_start = max(0, start - context_padding)
        padded_end = min(len(audio) / sr, end + context_padding)
        
        start_sample = int(padded_start * sr)
        end_sample = int(padded_end * sr)
        
        return audio[start_sample:end_sample], sr
    
    def _save_temp_audio(self, audio: np.ndarray, sr: int, segment_id: int) -> str:
        """Save audio chunk to temp file for Gemini upload."""
        temp_path = Path(self.temp_dir) / f"chunk_{segment_id}.wav"
        sf.write(str(temp_path), audio, sr)
        return str(temp_path)
    
    def _upload_audio_to_gemini(self, audio_path: str) -> Optional[object]:
        """Upload audio file to Gemini."""
        try:
            audio_file = genai.upload_file(audio_path)
            # Wait for processing
            while audio_file.state.name == "PROCESSING":
                time.sleep(0.5)
                audio_file = genai.get_file(audio_file.name)
            
            if audio_file.state.name == "ACTIVE":
                return audio_file
            else:
                print(f"   ‚ö†Ô∏è Audio upload failed: {audio_file.state.name}")
                return None
        except Exception as e:
            print(f"   ‚ö†Ô∏è Audio upload error: {e}")
            return None
    
    def _format_conversation(self, segments: List[Dict]) -> str:
        """Format segments th√†nh text."""
        lines = []
        for seg in segments:
            speaker = seg.get("speaker", "?")
            text = seg.get("text", "")
            seg_id = seg.get("id", 0)
            start = seg.get("start", 0)
            end = seg.get("end", 0)
            lines.append(f"[{seg_id}] [{start:.1f}s-{end:.1f}s] {speaker}: \"{text}\"")
        return "\n".join(lines)
    
    def _rule_based_label(self, text: str) -> Tuple[str, float, str]:
        """Fallback rule-based labeling v·ªõi extended taxonomy."""
        text_lower = text.lower().strip()
        words = text_lower.split()
        
        # Backchannel: ng·∫Øn v√† ch·ª©a marker
        if len(words) <= 3:
            if any(bc in text_lower for bc in self.BACKCHANNEL_WORDS):
                return "BACKCHANNEL", 0.8, "rule: short + backchannel word"
        
        # Interrupt markers
        for im in self.INTERRUPT_MARKERS:
            if im in text_lower:
                return "COMPETITIVE_INTERRUPT", 0.6, f"rule: contains '{im}'"
        
        # Yield: k·∫øt th√∫c b·∫±ng marker
        if words:
            last_word = words[-1]
            if any(last_word.endswith(ym) for ym in self.YIELD_MARKERS):
                return "YIELD", 0.7, f"rule: ends with '{last_word}'"
        
        # Backchannel: r·∫•t ng·∫Øn
        if len(words) <= 2:
            return "BACKCHANNEL", 0.6, "rule: very short utterance"
        
        # Hold: ch·ª©a hold marker ·ªü cu·ªëi
        for hm in self.HOLD_MARKERS:
            if hm in words[-3:]:
                return "HOLD", 0.6, f"rule: contains '{hm}'"
        
        # Default
        return "YIELD", 0.5, "rule: default"
    
    def _validate_label(self, label: str) -> str:
        """Validate and normalize label."""
        label = label.upper().strip()
        if label in self.VALID_LABELS:
            return label
        # Fuzzy matching
        if "COOP" in label or "H·ªñ TR·ª¢" in label:
            return "COOPERATIVE_INTERRUPT"
        if "COMP" in label or "C∆Ø·ªöP" in label:
            return "COMPETITIVE_INTERRUPT"
        if "BACK" in label:
            return "BACKCHANNEL"
        if "HOLD" in label or "GI·ªÆ" in label:
            return "HOLD"
        return "YIELD"
    
    def label_segments_multimodal(
        self,
        segments: List[Dict],
        audio: np.ndarray,
        sr: int,
        chunk_size: int = 5  # Smaller chunks for multimodal
    ) -> List[Dict]:
        """Label segments using multimodal (audio + text)."""
        print(f"   üéµ Multimodal labeling ({len(segments)} segments)...")
        
        for i in range(0, len(segments), chunk_size):
            chunk = segments[i:i + chunk_size]
            
            try:
                # Get time range for this chunk
                chunk_start = chunk[0].get("start", 0)
                chunk_end = chunk[-1].get("end", 0)
                
                # Extract audio chunk
                audio_chunk, _ = self._extract_audio_chunk(
                    audio, sr, chunk_start, chunk_end, context_padding=2.0
                )
                
                # Skip if too short
                if len(audio_chunk) < sr * 0.5:
                    print(f"      ‚è≠Ô∏è Chunk {i}-{i+len(chunk)} too short, using rules")
                    for seg in chunk:
                        label, conf, reason = self._rule_based_label(seg.get("text", ""))
                        seg["auto_label"] = label
                        seg["confidence"] = conf
                        seg["label_reason"] = reason
                        seg["label_mode"] = "rule"
                    continue
                
                # Save and upload audio
                temp_path = self._save_temp_audio(audio_chunk, sr, i)
                audio_file = self._upload_audio_to_gemini(temp_path)
                
                if not audio_file:
                    # Fallback to text-only
                    self._label_chunk_text_only(chunk)
                    continue
                
                # Format conversation
                conv_text = self._format_conversation(chunk)
                prompt = USER_PROMPT_MULTIMODAL.format(conversation=conv_text)
                
                # Call Gemini with audio + text
                response = self.model.generate_content(
                    [
                        SYSTEM_PROMPT_MULTIMODAL,
                        audio_file,
                        prompt
                    ],
                    generation_config=self.generation_config
                )
                
                # Parse response
                labels = json.loads(response.text)
                
                # Merge labels
                label_map = {l["segment_id"]: l for l in labels}
                for seg in chunk:
                    seg_id = seg["id"]
                    if seg_id in label_map:
                        lbl = label_map[seg_id]
                        seg["auto_label"] = self._validate_label(lbl.get("label", "YIELD"))
                        seg["confidence"] = lbl.get("confidence", 0.7)
                        seg["label_reason"] = lbl.get("reason", "multimodal")
                        seg["intonation"] = lbl.get("intonation", "unknown")
                        seg["intensity"] = lbl.get("intensity", "normal")
                        seg["label_mode"] = "multimodal"
                    else:
                        label, conf, reason = self._rule_based_label(seg.get("text", ""))
                        seg["auto_label"] = label
                        seg["confidence"] = conf
                        seg["label_reason"] = reason
                        seg["label_mode"] = "rule"
                
                # Delete uploaded file
                try:
                    genai.delete_file(audio_file.name)
                except:
                    pass
                
                # Rate limit
                time.sleep(1.0)
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Multimodal error: {e}. Using fallback.")
                self._label_chunk_text_only(chunk)
        
        return segments
    
    def _label_chunk_text_only(self, chunk: List[Dict]):
        """Fallback text-only labeling for a chunk."""
        try:
            conv_text = self._format_conversation(chunk)
            prompt = USER_PROMPT_TEXT.format(conversation=conv_text)
            
            response = self.model.generate_content(
                [
                    {"role": "user", "parts": [SYSTEM_PROMPT_TEXT]},
                    {"role": "model", "parts": ["T√¥i hi·ªÉu. S·∫Ω ph√¢n t√≠ch v√† g√°n nh√£n."]},
                    {"role": "user", "parts": [prompt]}
                ],
                generation_config=self.generation_config
            )
            
            labels = json.loads(response.text)
            label_map = {l["segment_id"]: l for l in labels}
            
            for seg in chunk:
                seg_id = seg["id"]
                if seg_id in label_map:
                    lbl = label_map[seg_id]
                    seg["auto_label"] = self._validate_label(lbl.get("label", "YIELD"))
                    seg["confidence"] = lbl.get("confidence", 0.7)
                    seg["label_reason"] = lbl.get("reason", "text-only")
                    seg["label_mode"] = "text"
                else:
                    label, conf, reason = self._rule_based_label(seg.get("text", ""))
                    seg["auto_label"] = label
                    seg["confidence"] = conf
                    seg["label_reason"] = reason
                    seg["label_mode"] = "rule"
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Text-only error: {e}. Using rules.")
            for seg in chunk:
                label, conf, reason = self._rule_based_label(seg.get("text", ""))
                seg["auto_label"] = label
                seg["confidence"] = conf
                seg["label_reason"] = reason
                seg["label_mode"] = "rule"
    
    def label_segments(
        self,
        segments: List[Dict],
        audio_path: Optional[str] = None,
        use_multimodal: bool = True
    ) -> List[Dict]:
        """Main labeling function."""
        if use_multimodal and self.enable_multimodal and audio_path:
            try:
                audio, sr = librosa.load(audio_path, sr=16000)
                return self.label_segments_multimodal(segments, audio, sr)
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error loading audio: {e}. Using text-only.")
        
        # Text-only fallback
        print(f"   üìù Text-only labeling ({len(segments)} segments)...")
        for i in range(0, len(segments), 10):
            chunk = segments[i:i + 10]
            self._label_chunk_text_only(chunk)
        
        return segments
    
    def flag_for_review(self, segments: List[Dict], threshold: float = 0.7) -> List[Dict]:
        """ƒê√°nh d·∫•u segments c·∫ßn human review."""
        for seg in segments:
            needs_review = False
            
            if seg.get("confidence", 1) < threshold:
                needs_review = True
            
            text = seg.get("text", "")
            label = seg.get("auto_label", "")
            
            # Short text m√† kh√¥ng ph·∫£i backchannel
            if len(text.split()) <= 2 and label not in ["BACKCHANNEL"]:
                needs_review = True
            
            # Long text m√† l√† backchannel
            if len(text.split()) > 5 and label == "BACKCHANNEL":
                needs_review = True
            
            # Interrupt labels c·∫ßn x√°c nh·∫≠n
            if "INTERRUPT" in label:
                needs_review = True
            
            seg["needs_review"] = needs_review
        
        return segments


def process_file(
    input_path: str,
    output_path: str,
    labeler: MultimodalLabeler,
    audio_dir: Optional[str] = None,
    use_multimodal: bool = True
) -> Dict:
    """Process m·ªôt file JSON."""
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    segments = data.get("segments", [])
    
    # Find audio path
    audio_path = None
    if audio_dir and use_multimodal:
        audio_file = data.get("audio_file", Path(input_path).stem + ".wav")
        for search_dir in [audio_dir, Path(audio_dir) / "youtube"]:
            candidate = Path(search_dir) / audio_file
            if candidate.exists():
                audio_path = str(candidate)
                break
    
    # Label
    segments = labeler.label_segments(
        segments,
        audio_path=audio_path,
        use_multimodal=use_multimodal and audio_path is not None
    )
    segments = labeler.flag_for_review(segments)
    
    # Stats
    stats = {
        "YIELD": sum(1 for s in segments if s.get("auto_label") == "YIELD"),
        "HOLD": sum(1 for s in segments if s.get("auto_label") == "HOLD"),
        "BACKCHANNEL": sum(1 for s in segments if s.get("auto_label") == "BACKCHANNEL"),
        "COOPERATIVE_INTERRUPT": sum(1 for s in segments if s.get("auto_label") == "COOPERATIVE_INTERRUPT"),
        "COMPETITIVE_INTERRUPT": sum(1 for s in segments if s.get("auto_label") == "COMPETITIVE_INTERRUPT"),
        "needs_review": sum(1 for s in segments if s.get("needs_review")),
        "multimodal_count": sum(1 for s in segments if s.get("label_mode") == "multimodal"),
    }
    
    data["segments"] = segments
    data["label_stats"] = stats
    data["labeler_version"] = "v2_multimodal"
    
    # Save
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="LLM Pre-labeling v2 v·ªõi Gemini Multimodal (Audio + Text)"
    )
    
    parser.add_argument("--input", "-i", required=True, help="Input dir/file")
    parser.add_argument("--output", "-o", default="data/processed/labeled", help="Output dir")
    parser.add_argument("--audio-dir", "-a", help="Th∆∞ m·ª•c ch·ª©a audio g·ªëc (cho multimodal)")
    parser.add_argument("--model", default="gemini-1.5-flash", help="Gemini model")
    parser.add_argument("--api-key", help="Google API key")
    parser.add_argument("--multimodal", action="store_true", help="Enable multimodal (audio + text)")
    parser.add_argument("--text-only", action="store_true", help="Force text-only mode")
    
    args = parser.parse_args()
    
    use_multimodal = args.multimodal and not args.text_only
    
    if use_multimodal and not args.audio_dir:
        print("‚ö†Ô∏è --multimodal requires --audio-dir. Falling back to text-only.")
        use_multimodal = False
    
    # Init labeler
    try:
        labeler = MultimodalLabeler(
            model=args.model,
            api_key=args.api_key,
            enable_multimodal=use_multimodal
        )
    except ValueError as e:
        print(f"‚ùå {e}")
        sys.exit(1)
    
    print(f"üöÄ Mode: {'Multimodal (Audio + Text)' if use_multimodal else 'Text-only'}")
    
    input_path = Path(args.input)
    
    if input_path.is_file():
        output_file = Path(args.output) / input_path.name
        print(f"üìù Processing: {input_path.name}")
        stats = process_file(
            str(input_path), str(output_file), labeler,
            args.audio_dir, use_multimodal
        )
        print(f"   ‚úÖ {stats}")
    
    elif input_path.is_dir():
        json_files = list(input_path.glob("*.json"))
        print(f"üìÇ Found {len(json_files)} files")
        
        total_stats = {
            "YIELD": 0, "HOLD": 0, "BACKCHANNEL": 0,
            "COOPERATIVE_INTERRUPT": 0, "COMPETITIVE_INTERRUPT": 0,
            "needs_review": 0, "multimodal_count": 0
        }
        
        for i, json_file in enumerate(json_files, 1):
            print(f"\n[{i}/{len(json_files)}] {json_file.name}")
            
            output_file = Path(args.output) / json_file.name
            if output_file.exists():
                print("   ‚è≠Ô∏è  Already labeled, skipping")
                continue
            
            stats = process_file(
                str(json_file), str(output_file), labeler,
                args.audio_dir, use_multimodal
            )
            print(f"   ‚úÖ {stats}")
            
            for k, v in stats.items():
                total_stats[k] = total_stats.get(k, 0) + v
        
        print(f"\nüìä Total: {total_stats}")
    
    else:
        print(f"‚ùå Invalid input: {args.input}")
        sys.exit(1)


if __name__ == "__main__":
    main()
